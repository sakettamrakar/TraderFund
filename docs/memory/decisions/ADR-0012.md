# ADR-0012: Zero-Trust Data Ingestion

**Status**: Accepted
**Date**: 2026-01-22
**Context**: External data sources (Alpha Vantage, Angel One) are **UNTRUSTED** by default. They can send bad data (NaN, Infinity, Gaps, incorrect timestamps) that corrupts downstream logic (Regime, Strategies). We need a gatekeeper.

## Decision

The system implements a **Zero-Trust Data Ingestion** pipeline. All inbound data is processed through **explicit validation gates**:

- **Schema Check**: All fields present and typed correctly.
- **Range Check**: `High >= Low`, `Volume >= 0`, `Price within acceptable bounds`.
- **Continuity Check**: No unexpected gaps (weekend/holiday logic).
- **Drift Check**: Price change < Circuit Breaker threshold.

Only **Validated** data is promoted to the **Canonical Store** (`processed/`). Raw data (`raw/`) is kept purely for audit/debug but NEVER used directly for decisions.

## Rationale

- **Resilience**: Prevents "Garbage In, Garbage Out". Strategies operate on trusted, clean data.
- **Fail-Fast**: Bad data triggers immediate rejection/alert, not silent corruption.
- **Auditability**: We know exactly *what* failed (e.g., "Angel One sent NaN for Reliance Open").

## Alternatives

- **Trust but Verify**: Load raw data, check later. Rejected (risks downstream corruption).
- **Manual Clean**: Let humans fix data. Rejected (unscalable).
- **Assume Good**: Trust the API. Rejected (APIs fail).

## Consequences

- **Latency**: Validation adds a step to the pipeline.
- **Complexity**: Requires maintaining strict schemas (JSONL -> Parquet) and validation logic.
- **Staleness**: If validation fails, the system may run on stale data ("Honest Stagnation") rather than consume bad data.

## Evidence

- `docs/memory/09_security/trust.md`: ยง2.2 Data Trust
- `docs/memory/04_architecture/data_flow.md`: Ingestion Pipeline (L0)
- `docs/verification/historical_ingestion_verification.md`: Validation Rules
